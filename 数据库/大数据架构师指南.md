# 大数据架构师指南

理解大数据需要三个层次的只是

1. 大数据是什么，《以大数据时代》为代表
2. 端到端的进行大数据方案涉及，如何理清重点
3. 大数据相关的基础技术知识

---

大数据技术框架

<img src="C:\Users\Leesure\AppData\Roaming\Typora\typora-user-images\image-20220315195621905.png" alt="image-20220315195621905" style="zoom:67%;margin-left:0px" />

- 大数据的4V特点

  1.海量化， 数据体量巨大

  2.多样化，数据类型日益繁多，传统的数据是二维表的形式存储在数据库中（又叫结构化数据）但是目前非结构化的数据成了数据的主要组成比分。

  3.快速化，是大数据必须满足的要求。对大量数据的快速处理与分析，是实现这一目标的前提

  4.价值化，大数据蕴含的整体价值是巨大的，但是由于干扰信息多，导致其价值密度低

- 大数据下，思维方式的变化

  1.全样本，大数据将使用更多的数据，甚至是全部数据进行分析，不再采用随机样本

  2.概率化，大数据允许劣质数据混杂其中，用概率来表示事物发展的大方向

  3.相关性，大数据更关心相关联系，因果关系被放在次要位置。在很多情况下，“是什么”比“为什么”对决策的帮助更大。

- 从技术层面将，以下几个方面是大数据的热点

  1. **硬件对架的冲击**，硬件性能的提升，会推动大数据系统架构的变革，以达到充分利用硬件，大幅度提升性能的目的
  2. **计算框架**。未来的计算框架以通用的计算框架为主（Spark可能成为主流）在特殊场景下为辅的专业计算框架
  3. **数据封装的中间件**。大数据中间件是位于应用层和底层数据库之间的组件，屏蔽掉不同数据库之间的差异，同时为上层应用提供统一的开发接口。
  4. **非结构化数据处理**。
  5. **智慧发现**。大数据与人工智能结合
  6. **可视化**。可视化的三个趋势：1.扁平化；2.动态化，交互；3.多维度，多图联动。

# 集群相关概念

## 1. 负载均衡LB

负载均衡（Load Balance,LB）

单台负载均衡器位于网站的最前端，起着对客户请求分流的作用，相当于整个网站或系统的入口。如果只有一台负载均衡器，若负载均衡器崩溃了，则整个网站也会挂掉。

## 2. 高可用HA

高可用（High Availability, HA) 有两个不同的含义，广义上，是指整个系统的高可用性，即整个系统不会因为某一台主机崩溃或故障而导致整个系统停止服务的现象。狭义上就是指主机的冗余接管。

# 大数据架构基础

## Hadoop 基础

官方定义： $Apache^{TM} Hadoop$ 是一套可靠的，可扩展的，支持分布式计算的开源软件。

> 特点

1. 能够对大量数据进行分布式处理的开源软件框架
2. 分布式计算平台
3. 使用java编写，但是基于Hadoop的应用也可以使用其他语言编写

### 1.Hadoop版本

当前Hadoop有两大版本： Hadoop1.0 和Hadoop2.0，其中Hadoop是比较 稳定的版本，是目前业界主流使用的Hadoop版本

![image-20220319155433872](../../AppData/Roaming/Typora/typora-user-images/image-20220319155433872.png)

#### 1. Hadoop 1.0

---

HDFS: 分布式文件系统

- 由一个NameNode 和多个DataNode 组成

---

MapReduce：分布式计算框架

- 由一个JobTracker和多个TaskTracker组成

#### 2.Hadoop 2.0

Hadoop2.0的生态系统为：

![image-20220319162232422](../../AppData/Roaming/Typora/typora-user-images/image-20220319162232422.png)

核心项目包括：

- **HDFS** : 提供了高吞吐量的访问应用程序数据
- **Yarn:** 用于作业调度和集群资源管理
- **Map reduce** ：基于Yarn的大数据集的并行处理系统

其他子项目包括：

- **Ambari:**一个部署、管理和监视集群的开源框架，它提供了一个直观的操作工具和一个健壮的Hadoop API，简化集群操作
- **HBase**: 可扩展的分布式**列式**数据库，支持达标的结构化存储
- **Hive**:分布式数据仓库系统，提供基于**类SQL的查询语言**
- **Mathout**:机器学习和数据挖掘领域经典算法的实现
- **Pig**: 一个高级**数据流**语言和执行环境，用来检索海量数据数据集
- **Spark**: 快速通用的计算引擎
- **Sqoop**:在关系型数据库与Hadoop系统之间进行数据传输的工具
- **Tez**:是从MapReduce计算框架演化而来的通用DAG计算矿机啊，可以作为MapReduce/Pig/HIve等系统的底层数据处理引擎，天生融入Hadooop2.0的Yarn中
- **Zookeeper**: 提供集群的协调服务

### 2.HDFS简介

分布式文件系统，能提供高吞吐量的数据访问，适合大规模数据集上的应用。

HDFS设计的前提和设计目标：

- **硬件错误**。硬件错误是常态不是异常。大量集群中，任何一个组件都有可能失效。意味着总有一部分HDFS的组件是不工作的，**错误检测和快速、自动地回复是HDFS最核心的架构目标**。

- **流式的数据访问**。HDFS建立在这样的一个思想上：**一次写入，多次读取**。这样的模式是最高效的。一个数据集一旦由数据源生成，就会被复制分发到不同的存储节点中。然后响应不同的数据分析任务请求。在多数情况下，分析任务都会涉及数据集中的大部分数据，因此在HDFS中， 请求读取整个数据集要比读取一条记录更加高效。
- **处理超大文件**。HDFS在实际应用中，可以用来存储管理PB级别的数据。HDFS提供很高的聚合数据宽带，在一个集群中支持成千上百个节点，支持上亿级别的文件。
- **简化数据一致性问题**：由于大部分的HDFS程序对文件操作是一次写入，多次读取的模式。文件一旦创建，写入，关闭，就不需要再次修改了。这种模式简化了数据一致性的问题，并实现了高吞吐量地访问数据。
- **移动计算比移动数据更经济**：在靠近数据存储的位置来进行计算是比较快的，在数据量比较大的时候，进行计算可以消除网络的拥堵，提高系统整体吞吐量。HDFS提供了接口接口，让程序自己移动到离数据存储更近的位置。
- 可移植性：HDFS可以简单地实现平台间的迁移。

### 3.HDFS体系结构

`HDFS`是一个主从复制的结构`Master/Slave` 体系结构，一个`HDFS`是由一个`NameNode`和一定数目的`DataNode`

- `NameNode`管理文件系统的元数据

- `DataNode`存储实际的数据

  从内部看，一个文件被分成一个或者多个数据块，这些块存储在DataNode上，客户端与NameNode交互以获取文件的元数据，真正的文件I/O操作是直接和DataNode进行交互的。

  > HDFS架构图

![image-20220320120209573](../../AppData/Roaming/Typora/typora-user-images/image-20220320120209573.png)

- `NameNode`（元数据节点） 执行文件系统的命名空间操作，如打开，关闭，重命名文件，重命名目录。负责确定数据块到具体DataNode节点的映射。
  - NameNode 可以看作是分布式文件系统中的管理者，负责管理文件系统的命名空间，以及客户端对文件的访问
  - NameNode将所有的文件和文件夹的元数据保存在一个文件系统树种。这些信息回在硬盘上保存成以下文件：命名空间镜像（img）、操作日志（editlog）。此外还保存了一个文件包括哪些数据块，分布在哪些数据节点上，但是这些信息不会存储在硬盘上，而是在启动的时候，由DataNode上报到NameNode。
  - 用户数据永远不会流过NameNode
- `DataNode`（数据节点）负责处理文件系统客户端的读写请求。在NameNode的统一调度下进行数据块的创建，删除和复制
  - DataNode是文件系统种真正存储数据的地方，DataNode负责处理文件系统客户端的读写请求。
  - 在NameNode的统一调度下进行数据块的创建，删除和复制。
  - 客户端可以向DataNode请求写入或者读取数据块。
  - DataNode周期性地向NameNode节点汇报器存储的数据块信息。
  - DataNode是文件存储的基本单元，它将Block存储在本地文件系统中，保存了Block的元数据，同时周期性地将所有存在的Block信息发送给NameNode
- `Client`是HDFS文件系统的客户端
  - 应用程序通过客户端与NameNode, DataNode交互，进行实际的文件读写。

### 4.HDFS数据复制

HDFS在集群中大量机器之间存储大量文件，以块序列的形式存储文件。文件中除了最后一个块，其他块都有相同的大小，默认128M一个数据块。如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。

为了容错，文件的所有块都会有副本，每个文件的数据块大小和副本数量都是可以配置的，应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，后续也可以调整。

HDFS中的文件是一次写的，并且任何时候都只有一个写操作。

NameNode负责处理所有的块复制的相关决策，它周期性的接受集群中数据节点的心跳信号和块报告。

接收心跳信号表示该DataNode节点工作正常；块状报告包含了一个当前DataNode上所有数据块的列表。

> 副本位置

块副本存放位置的选择是HDFS可靠性和性能的关键。HDFS采用一种称为**机架感知（rack-aware）的策略**来改进数据的可靠性

HDFS运行在跨越大量机架的集群之上。两个不同机架上的节点是通过**交换机**实现通信的，大多数情况下，相同机架上机器间的宽带尤与在不同机架上的机器。

通过**机架感知**的过程，NameNode可以确定每个DataNode所属的机架id，在开始的时候，每一个DataNode自检所属的机架id，然后在向NameNode注册的时候，告知它的机架id。

---

​		一个简单的但不是最优的复制方式就是**将副本放置在不同的机架上**，

- 优点：这样就防止了机架故障时数据的丢失，并且在**读数据**的时候可以从分利用不同机架的宽带。这种方式均匀地将复本分散在集群中，简单地实现了组建故障时的负载均衡。
- 缺点： 但是这种方式增加了写的成本，因为写的时候需要跨越多个机架传输文件块。

---

​		默认的HDFS block防止策略在最小化写开销和最大化读取中进行了一些折中。

​		一般情况下，复制因子为3。HDFS的副本放置策略是将第一个副本放置在本地节点，将第二个副本放到本地机架的另外一个节点，将第三个副本放到不同机架上的节点。

这种方式减少了机架间的写流量，提高了写性能，减少了读操作的网络聚合宽带。因为文件的副本块仅存在两个不同的机架，而不是三个。这种策略并不是均匀分布在不同机架上。1/3的副本在一个节点上，2/3的副本在一个机架上。其他副本均匀分布在剩下的机架中

> 副本选择

为了降低整体的宽带消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。假如在读节点的同一个机架上就有这个副本，则直接读取。如果HDFS集群跨越多个数据中心，那么客户端将首先读本地数据中心的副本

### 5. 总结

HDFS有如下技术特点和应用场景：

1. 适合处理超大文件，数量级达到GB,TB,PB级别
2. 支持集群规模的动态扩展
3. 适用于流式读写的场景，即“一次写入，多次读取”
4. 具有高容错性，数据块可以保存多个副本，实现负载均衡
5. 对硬件要求较低，能够运行在廉价的商用机器集群上

HDFS不适合的应用场景如下：

1. 不适合高效存储大量小文件的场景
2. 不适合低延迟的数据访问场景
3. 不适合多用户同时写入和任意修改文件的场景

## YARN

​		YARN(Yet Another Resource Negotiator)是一个通用的资源管理平台，可**为各类计算框架提供资源的管理和调度**。

YARN可以将多种计算框架(如离线处理的MapReduce、在线处理的Storm、内存计算框架Spark)部署到一个公共集群中，共享集群的资源，Yarn提供的功能如下。

1. **统一的资源管理和调度**。集群中的所有节点的资源（内存，CPU，磁盘、网络）抽象为容器（Container）在资源进行运算任务时，计算框架需要向YARN申请Container，YARN按照策略对资源进行调度，进行Container的分配。
2. **资源隔离**。YARN使用了轻量级**资源隔离机制**（Cgroup）进行资源隔离，以避免相互干扰，一旦Container使用的资源量超过预定的限值，则将结束该任务

YARN可以看作是一个云操作系统，由一个ResourceManager和多个NodeManager组成，YARN负责管理所有NodeManager上多维度资源，并以Container（启动一个Container相当于启动一个进程）方式分配给应用程序启动ApplicationMaster(相当于主进程)或运行ApplicationMaster切分的各Task(相当于子进程)

### 1. YARN的体系结构

YARN是Master/Slave结构，主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件组成。结构如下所示。

![image-20220320151745466](../../AppData/Roaming/Typora/typora-user-images/image-20220320151745466.png)

1. **ResourceManager(RM)**：负责对各NodeManager上的资源进行统一管理和调度。给ApplicationMaster分配空闲的Container并监控其状态，对AM申请的资源请求分配相应的空闲Container。其主要由两个组件构成：调度器和应用程序管理器。
   1. 调度器。调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。调度器仅仅根据各个应用程序的资源需求进行分配，分配的单位是Container,限定每个任务使用的资源量
   2. 应用程序管理器。应用程序管理器负责管理整个系统中所有的应用程序，包括应用程序提交，与调度器协商资源以启动ApplicationMaster，监控AM的运行状态并在失败时重启AM。
2. **NodeManager(NM)**:NM是每个节点上的资源和任务管理器。它会定时地向**RM**汇报本节点上的资源使用的情况和各个Container的运行状态；同时会接收处理来自AM的Container启动、停止请求。
3. **ApplicationMaster(AM)**:用户提交的应用程序包含一个AM,负责应用的监控，跟踪应用执行状态，重启失败任务等
4. **Container**:Container封装了某个节点上的多维度资源，如内存，CPU,磁盘，网络。是YARN对资源的抽象，当AM向RM申请资源时，RM为AM返回的资源是用Container表示的。YARN会为每个任务分配一个Container，该任务只能使用Container中包含的资源。

### 2.YARN应用工作流程

客户端向YARN提交一个应用程序后，YARN将**分两个阶段**运行该应用程序：

1. 启动AM
2. 由AM创建应用程序，申请资源并监控其整个运行过程，直到运行完成。

YARN的工作流程如图



YARN的巩固走流程具体过程为：

1. 用户向YARN提交应用程序，其中包括AM程序，启动AM的命令，用户程序等。
2. RM为该应用程序分配第一个Container, 并与对应的NM通信，要求它在这个Container中AM
3. AM首先向RM注册，这样用户可以直接通过RM查看应用程序的运行状态，然后AM将为各个任务申请资源，监控任务的运行状态，直到运行结束（重复步骤4-7）

4. AM通过轮询的方式通过**RPC协议**向RM申请资源和领取资源
5. AM申请到资源后，便与对应的NM通信，要求NM启动任务
6. NM为任务设置好运行环境（包括环境变量、JAR包、二进制程序），将任务启动命令写到一个脚本中，并通过运行脚本启动任务
7. 各个任务通过RPC协议向AM汇报自己的状态和进度，让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。
8. 应用程序运行完成后，AM向RM注销并关闭自己

### 3.YARN资源调度模型

YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container

YARN采用了双层资源调度模型：

- RM中的资源调度器将资源分配给各个AM:资源分配过程是异步。RM中的资源调度器将资源分配给一个应用程序后，不会立刻push给对应的AM， 而是暂时放到一个缓冲区中，等待AM通过周期性的心跳主动来获取
- AM领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。

---

YARN目前采用的资源分配算法有三种：

1. **先来先调度FIFO:**按照优先级高低调度，如优先级相同则按照提交时间先后顺序调度，如果提交时间相同则按照队列名或Application ID比较顺序调度
2. **公平调度FAIR**：该算法的思想是尽可能公平地调度，已分配资源少的优先级高
3. **主资源公平调度DRF**: 该算法扩展了最大最小公平算法，使之能够支持多维资源，算法是匹配资源百分比小的优先级高

### 4.总结

YARN是一个可为各类计算框架提供资源管理和调度的通用资源管理平台。YARN的使用场景如下：

1. YARN使用了轻量级资源隔离机制Cgroup进行资源隔离，以避免资源之间相互干扰，实现对CPU和内存两种资源的隔离
2. YARN上可以运行各种应用类型的计算框架，包括离线计算MapReduce, 基于内存的计算框架Spark，实时计算框架Storm，DAG计算框架Tez
3. 支持FIFO，FAIR，DRF调度算法
4. 支持租户资源调度，包括资源按比例分配，层级队列划分和资源抢占

## MapReduce

MapReduce致力于解决大规模数据处理的问题，利用局部性原理将整个问题分而治之。

MapReduce在处理之前，将数据集分布至各个节点。处理时，每个节点就近读取本地存储的数据（Map)，将处理后的数据进行合并（Combine）、排序（Shuffle and Sort）后再分发至Reduce节点。避免了大量数据的传输。配合数据**复制Replication策略**，集群可以具有良好的容错性，一部分节点的宕机对集群的工作不会照成影响。

### 1.体系结构

MapReduce作为分布式计算框架，主要有三部分组成：

1. **编程模型**。为用户提供了非常易用的编程接口，用户只需要考虑如何使用MapReduce模型描述问题，实现几个hook函数即可实现一个分布式程序
2. **数据处理引擎**。由MapTask和ReduceTask组成，分别负责Map阶段和Reduce阶段的逻辑处理
3. **运行时环境**。用来执行MapReduce程序，并行执行程序的诸多细节。如分发，合并，同步，检测等功能均交由执行框架负责。

:warning:MapReduce不适合实时应用的需求，MapReduce本质上是一种线性的编程模型，适用于顺序处理数据。

### 2. `MapReduce`可编程组件

- MapReduce提供了5个可编程组件，可编程组件全部属于回调接口。当用户按照约定实现这几个接口后，MapReduce运行时环境会自动调用实现计算

![image-20220320180939320](../../AppData/Roaming/Typora/typora-user-images/image-20220320180939320.png)

1. **InputFormat**:主要用于描述输入数据的格式，其按照某个策略将输入数据切分成若干个split,并为Mapper提供输入数据，将split解析成一个个key:value对
2. **Mapper**：对split传入的key:value对进行处理，产生新的键值key2:value2。即$Map:(k1, v1)\rightarrow(k2,v2)$ 
3. **Partitioner**:作用是对Mapper产生的中间结果进行分区。以便将key有耦合关系的数据交给同一个Reduce处理，它直接影响Reduce阶段的负载均衡。
4. **Reducer**:以Mapper的输出作为输入，对其进行排序和分组，再进行处理产生新的数据集，即$Reducer:(k1,list(v2))\rightarrow(k3,v3)$ 
5. **OutputFormat**：主要用于描述输出数据的格式，它能够将用户提供的key/value对写入特定格式的文件中

---

编程流程的运行流如下：

1. 作业提交后，InputFormat按照策略将输入数据切分成若干个split
2. 各Map任务节点上根据分配的split元信息获取相应数据，并将迭代解析成一个个`key/value对`
3. 迭代的`key/value对`由Mapper处理称为新的`key1/value1对`
4. 新的`key1/value1对`先进行排序，然后由Partitioner将有耦合关系的数据分到同一个Reducer上进行处理，中间数据出入本地磁盘
5. 各Reduce任务节点根据已有的Map节点远程获取数据，只获取属于该Reduce的数据，该过程被称为shuffle
6. 对数据进行排序，并进行分组（将相同key的数据分为一组）
7. 迭代key/value对，并由Reducer合并处理为新的key2/value2对
8. 新的key2/value2对由OutputFormat保存到输出文件中

### 3.MapReduce数据处理引擎

在MapReduce计算框架中，一个Job被划分为Map和Reduce两个计算阶段，这两个阶段分别由多个`Map Task`和`Reduce Task`组成。这两种服务构成了MapReduce数据处理引擎。

---

`MapTask`的整体计算流程共为5个阶段：

1. Read阶段。MapReduce通过用户编写的RecoderReader，从输入InputSplit中解析出一个个key/value
2. Map阶段。将解析出的key/value交给用户编写的Map函数处理，并产生一系列新的key/value
3. Collect阶段。Map函数生成的Key/value通过调用Partitioner进行分片，并写入一个环形内存缓冲区中
4. Spill阶段。即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件
5. Combine阶段。所有的数据处理完成后，Map Task对所有的临时文件进行一次合并，最终只会产生一个数据文件。

---

`Reduce Task`的整体计算流程共为5个阶段：

1. Shuffle阶段。Reduce Task从各个Map Task远程拷贝一片数据，并针对某一片数据进行读取。如果数据大小超过一定阈值则写在磁盘上，否则直接放在内存中
2. Merge阶段。在远程拷贝数据的同时，Reduce Task启动了3个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上的文件过多
3. Sort阶段。采用了基于排序的策略将key相同的数据聚在一起，由于各个Map Task已经对结果进行了局部排序。因此Reduce Task只需对所有数据进行一次归并排序即可。
4. Reduce阶段。将每组数据依次交给用户编写的Reduce函数处理。
5. Write阶段。将reduce函数的处理结果写到HDFS上

:cherry_blossom:MapReduce有两个版本：MRV1和MRV2。MRV1仅仅是一个独立的离线计算矿机啊，而MRV2则是运行在YARN之上的MapReduce应用，每一个作业都有一个应用Application Master。

MRV2解决了MRV1的扩展性差，可靠性差，资源利用率低等问题，同时兼容了MRV1的API

### 4.总结

1. MapReduce的计算延迟较高，对实时性要求比较高的应用场景不适合使用MapReduce
2. MapReduce适合顺序批量处理数据，处理随机访问的能力不足，因此不适合处理随机数据的场景

## ZooKeeper

ZooKeeper是一个分布式系统的协调系统，通过协调机制来统一各个系统的状态。ZooKeeper的体系架构如下所示。

![image-20220321155311929](../../AppData/Roaming/Typora/typora-user-images/image-20220321155311929.png)

客户端可以连接任何一个Server，每个Server的数据完全相同，每一个Follower都和Leader有连接，接收Leader的数据更新操作，Server记录事务日志和快照到持久存储。如果过半的Server可用，则整个服务就可用。Leader只有一个，Leader宕机后就会重新选出一个Leader

Client无论连接到哪个Server，展示给它的都是同一个试图。这是Zookeeper的重要功能